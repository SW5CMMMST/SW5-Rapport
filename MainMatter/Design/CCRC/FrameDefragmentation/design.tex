%!TEX root = ../../../../master.tex
\section{Frame Defragmentation}
A significant issue for long-lived \gls{tdma} networks such as the one constructed is fragmentation caused by removal of one or more devices. 
The term fragmentation refers to unused parts as one or more parts of a sequential resource. 
In relation to \gls{tdma} it is an unused time-slot in the frame. 
Currently in the design if a device leaves and joins again, it will get a new time-slot in the frame, and the old time-slot will remain unused, thus causing the frame to be longer and therefore slowing the network.
We make two distinctions; \gls{erp} and \gls{irp}.
\gls{erp} is when a device would inform other devices of its impeding removal, e.g. powering-off soon. 
\gls{irp} is if a device is suddenly unable to transmit for any other reason, i.e. suddenly powering-off, transmission module failure, etc. it is unable to inform the other devices of this ahead of time. 

A solution to this problem should ideally handle both of these sub-problems, however a solution to the \gls{irp} will also solve the \gls{erp}.
Therefore it is sufficient to solve \gls{irp}, which means that the rest of this chapter will only focus on \gls{irp}.  
The design in this chapter inherits the assumptions of the \gls{ccuc}-problem, however to get an idea of the different solutions performance we will consider them in respect to the \gls{ccrc}-problem. 

\subsection{Methods of defragmenting a network}

The value $m_{max}$ is the number of times no device received anything from a given device, after which it is considered inactive. 
In \gls{ccrc} $m_{max} = 1$ should be a possibility if one assumes that the only cause of not receiving transmissions is that a device is offline, and no jamming will occur since a network of more than one device is established.
In \gls{ccuc} and in practice however, it would be risky using this assumption.
This leads to a design where a device should have consensus from a majority of the network as is mentioned in \myref{consensus}.

\subsubsection{Decremental Gradual Defragmentation}
This design moves devices to the previous slot if it is empty. 
Thereby gradually reducing the fragmentation of the frame.
The proposed design has a special case for the first device in the frame, and a general case for the rest of the devices.

\paragraph{The General case}applies to any device which is part of a network and not the first device in the frame, i.e. $k \neq 1$.
If the device does not receive any payload from the device before it, i.e. device $k - 1$, a given number of times in a row, called $m_{max}$, then the device should ask other devices for consensus on the potentially faulty device.
If a consensus can be established that the device is not transmitting all devices should be notified that the current device will transmit in the time-slot of the faulty device from which no one received any transmission, instead of its own time-slot in the next frame and onward. 
This will create another missing slot to be handled by another device, however a problem arises when the unused time-slot is $n - 1$, i.e. the last time-slot of the frame, which device should take this time-slot?
This will will be explained in the following paragraph.

\paragraph{The Special case}applies to the first device of the frame, i.e. $k = 1$. 
Since there is no device in the same frame before the first device, which time-slot it can move to, it should instead reduce the number of time-slots in the frame by one, if it does not receive any payload from the last device of the frame, i.e. $k = n - 1$, a given number of times, $m_{max}$, and gets consensus from the network. 

\tikzfigure{dgd.tikz}{Decremental Gradual Defragmentation, removal of device in slot 2.}{dgd} 

\bigskip \noindent
This is a simple solution, which defragments the frame one device at a time, the worst case scenario is that the first device of the frame is unable to transmit, then the second device will take its spot, and so on until the new first device reduces the amount of time-slots in the frame by one. 
This will always take $(n - k) \times m_{max}$ frames, where $k$ is the time-slot with the disconnected device, assuming it is the only change to the network in the meantime. 
The device reducing the time-slots in the frame by one will not cause new devices to be unable to join the network, as the free time-slot will simply be placed in the previous time-slot. 

\subsubsection{Jumping Defragmentation}
Jumping defragmentation reduces fragmentation by moving the last device of a frame to an unused time-slot. Further if the last time-slot is unused then the first device should reduce the size of the frame.

\paragraph{The last device} of the frame should monitor which time-slots are unused for $m_{max}$ time-slots and then get consensus from the network. 
If such a time-slot is found, then the last device should take that time-slot and reduce the size of the frame by one. 
It is preferred to be the first available time-slot, but is not a strict necessity.   

\paragraph{The first device} of the frame should monitor if the last time-slot of the frame is unused for $m_{max}$ time-slots.
If that is the case then it should reduce the size of the frame by one.
This happens if the last device of the frame is suddenly missing.

\tikzfigure{jd.tikz}{Jumping Defragmentation, removal of device in slot 2.}{jd} 

\bigskip \noindent
This reduces fragmentation as any single unused time-slot will be used in the next frame.
The other devices will detect that the size of the frame has decreased and use the new size of the frame. 
If multiple devices are removed at the same time, the last device should take any of the empty time-slots, then another device will be the last device of the next frame, then this other device can do the same in the frame after that. 
This has the advantage over Decremental Gradual Defragmentation that it is faster, any single device removal will be solved in the next frame, where in Decremental Gradual Defragmentation it takes $(n - k) \times m_{max}$ frames to defragment the frame. 
Where as in the same case Jumping Defragmentation takes $m_{max} \times 2$ frames to defragment, $m_{max}$ for detecting and jumping and $m_{max}$ for the decreasing n. 

\subsubsection{Automatic Insertion in Unused Time-slots}
Another solution is for new devices to insert themselves in unused time-slot of an existing network.

\paragraph{Addition to start-up}
This would change the start-up procedure to require new devices, which discovers a network, to listen for at least $m_{max}$ frames then insert themselves in an unoccupied time-slot, and if no unoccupied time-slot is found then in the empty slot.
A device would then wait for confirmation that it, indeed had taken an empty slot or otherwise leave the network and try again.

\tikzfigure{aiut.tikz}{Automatic insertion in slot 2.}{aiut} 

\bigskip \noindent
This will only in some cases resolve fragmentation from a network as one cannot guarantee that a new device will connect. 
For example the case in which a device restarts unexpectedly, then it would retake its old time-slot, assuming no other parts of the network changed in the downtime.
On the other hand in the case where the device is effectively removed, the fragmentation will persist until a new device is added, which depending on the application could be a long time.
It is a relatively simple change to the existing protocol, and would therefore be easy to implement however as it does not solve the problem fully, it is not used. 

\subsection{Changes to the Protocol}
To evaluate the methods above the following factors will be considered: Speed and ability to defragment.
The Decremental Gradual Defragmentation-method will defragment the network, however it is slow, as it takes $(n - k) \times m_{max}$ frames to defragment, where $k$ is the time-slot which the device  that was removed had. 
The Automatic Insertion in Unused Time-slots-method increases the time it takes to connect a new device by $m_{max} \times n$ time-slots from a new device is turned on as it must confirm that the new device is actually dead.
This could be reduced by allowing the network to transmit that device $k$ is dead.
However it will not defragment a network which decrease in size. 
The last method: Jumping Defragmentation is fast, it will only take $m_{max} \times 2$ frames to defragment any single device leaving the network, and defragments the network in all cases. 
By comparison the Jumping Defragmentation-method is seen as the best of the above solutions to add to the existing protocol. 

\bigskip \noindent
An issue that has not yet been touched upon in regards to defragmentation is the reduction of $n$ in an unreliable network.
For it to work flawlessly every single device would have to reduce $n$ in the same frame.
As the communication is unreliable, a device may not receive the payload calling for the reduction of $n$ and thus have a different $n$.
The chance of this can be reduced by transmitting that in e.g. 10 time frames $n$ will be reduced however there will still be a chance for one device not receiving that payload for 10 frames.
As such for defragmentation to work in a \gls{ccuc}-network one must also develop a method which guarantees that a network broken by an inconsistent $n$ will fix itself.