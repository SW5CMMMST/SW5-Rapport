\chapter{Discussion}\label{Discussion}
This chapter will reflect on the choices made throughout this paper in respect to the problem statement in \myref{sec:problemStatement}, which was derived from wanting to communicate on the same frequency in simpler systems, as the number of frequencies in Wi-Fi are limited, and at some point communicating on the same frequency might be necessary.
Following this chapter the \myref{conclusion} will conclude on the success, or lack thereof, for these choices.

%Analysis and hardware test
\bigskip \noindent
Prior to development both hardware and single-frequency communication methods were taken into account.
For communication the choice ended on TDMA, while there exists enough methods to fill this entire paper with just that, most are merely mutations of TDMA adapted to more specific scenarios, as such TDMA was chosen as the primary inspiration for the protocol development.
As for the hardware several embedded systems were considered including the popular Raspberry Pi yet the choice still fell onto Arduino as it fit better in the scope of the project due to price, hardware specifications and availability.

Tests were run on the hardware showing just how unreliable the wireless technology in the RF-modules was.
To counter this an antenna was attached adhering to the physics of radio communication, this was further tested against an antenna of random length and proved to be significantly better than both no antenna and the antenna of random length.
Any multiple of the chosen antenna length would fit according to the physics used in \myref{rfmodule}.
This however would be unnecessary as the antenna proved to work stably upwards to 28 meters without any significant change in package loss.
28 meters happens to be more than enough when applications considered are alarms, home automation or similar systems.

%Sub-problem split and dilemma
\bigskip \noindent
Following the hardware analysis and testing the problem was split into sub-problems all represented using graph theory.
The four sub-problems were the result of considering two variables, communication and device placement according to which devices they are in range of.
While the device placement was a sensible consideration as the two scenarios, strongly and completely connected networks, are significantly different one can argue as to whether the communication consideration was well enough, or perhaps it should have been more thorough.
As it happens the CCUC-solution has not been fully implemented, but merely designed and somewhat tested in UPPAAL SMC.
This is partly due to unforeseen sub-iterations in the CCRC-solution pertaining to certain edge cases requiring entire design structures.

One of these in particular would cause a complication in when expanding to the CCUC-problem as a consequence of the consensus idea, the sub-iteration in question is removal of dead devices and by extension reduction of time-slots.
What really causes the problem here is that this requires that the value $n$, number of time-slots in the network, can both rise and fall, instead of only rise.
If one had spotted these edge cases in the initial partition of problems one could have defined which sub-iterations to consider for each problem such that sub-iterations which would require a significant change once expanding the problem, would not be designed in the former problem as to avoid a rework of an entire sub-iteration.

%General Design methodology
\bigskip \noindent
Throughout designing the protocol there was a heavy focus on developing and designing the solution theoretically completely rather than writing code once an idea was presented, and then solving whatever issues the idea may have had in the code.
This is reflected in the design chapters where there is a heavy focus on flow diagrams, pseudocode and model checking.

In order to develop a protocol for the CCRC-problem, with multiple device activation, both stochastic and definite ideas were considered.
Neither of the approaches were able to provide a solution which would work 100 \% of the time, however since the stochastic solution could not hit a scenario for which it would loop without a chance of working, it was chosen as the better approach.

By developing a complete theoretical solution and using a model checker the solution was able to be validated; ensuring, as long as the code fit the model, the code would work every time for the CCRC-problem and within a certain probability for the CCUC-problem.
The CCRC-problem being an implausible scenario, simply proves that the theoretical problem was solved, however for the CCUC-solution the model checking provides proof that the protocol works in most of the thousands of possible scenarios, something one could not prove through a practical example.

The alternative method of development for a solution would be developing through coding then proceeding to model check the already created code.
The work flow of doing this however would quite possibly increase the work done, as finding an error, edge case or other potential issues are significantly more difficult to do in 500 lines of code, rather than using a model checker on already designed pseudocode and flow diagrams.
One could combine the code with the model checker, but creating a model to check, is significantly more problematic to do from 500 lines of code rather than a flow diagram.

%CCUC Expansion
\bigskip \noindent
In expanding to the CCUC-problem several ideas on handling this were presented, both in regard to avoidance and damage control.
While only one of the avoidance ideas, redundant messaging, has been applied it is not a reason to disregard the rest as possible techniques.
In choosing an idea to use one must also consider the cost, for both of the ideas mentioned in \myref{AvoidanceCCUC} it becomes a strain on the time-slot length, thus for an application one might not want to use these particular ideas.
This is particularly worth considering if the network is already running with a high reliability, in that case it may be better to perform some sort of damage control on devices that become desynchronised, whether this is by restarting or using one of the designed damage control ideas. 

If the reliability is low, but reducing the time-slot length is of importance, it further becomes important to consider the impact of losing a package for a given application in order to decide whether increasing reliability by increasing time-slot length is worth it.
For the opposite scenario where reliability is of extreme importance it may also be worth increasing the time-slot length even further by sending the message more than twice while also implementing a damage control idea which is not to simply reset the device.

%Ending the chapter
\bigskip \noindent
Ideas for damage control were not the only things not implemented which had been designed, this is an unfortunate side effect of the time constraints on the project.
Ideas that fall under that category, as well as ideas for possible improvements are discussed further in \myref{futureWorks} and \myref{mortensStuff}\todo{Kilde her!!!}